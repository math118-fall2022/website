---
title: "Ultimate Candy Rankings - Model Selection"
date: "`r Sys.Date()`"
output: html_document
---

```{r}
library(tidyverse)
library(fivethirtyeight)
library(broom)
library(knitr)
```

- In your R Markdown file, load the fivethirtyeight package along with other packages required for analysis. 

- We will use the `candy_rankings` dataset for this analysis. Type `??candy_rankings` to see the variable names and definitions. 

### Planning

Decide on a subset of 7 - 10 variables to consider for your analysis. 

- This list should include at least two interaction terms. Remember, 
we consider interactions when variables might behave differently for various 
levels of another variable. Ideally, you would get expert guidance for
choosing interactions.

- The more variables you consider the longer model selection will take so keep that in mind.


### Model fitting, selection, and interpretation

- Use backwards elimination to do model selection. Make sure to show 
each step of decision (though you don't have to interpret the models at 
each stage).
- Yes, this is tedious. And yes, there are ways of automating it. But for now, 
  go through the process "manually", to get a good sense of how the model 
  selection algorithm works.
- Provide interpretations for the slopes for the final model you arrive at 
and create at least one visualization that supports your narrative.


### Example Solution 

Let's suppose I start with the variables: 

`chocolate`, `fruity`, `nougat`, `pricepercent`, `sugarpercent`, `sugarpercent*chocolate`, `pricepercent*fruity`

**Step 0: Full  Model**

```{r}
full <- lm(winpercent ~ chocolate + fruity + nougat + pricepercent + sugarpercent +
             sugarpercent*chocolate + pricepercent*fruity, data = candy_rankings)
glance(full)$adj.r.squared
```

**Step 1: All possible models removing 1 term at a time from the full model**

```{r}
#remove chocolate and it's associated interaction
step1 <- lm(winpercent ~ fruity + nougat + pricepercent + sugarpercent + pricepercent*fruity, data = candy_rankings)
glance(step1)$adj.r.squared
```

```{r}
#remove fruity and it's associated interaction
step1 <- lm(winpercent ~ chocolate + nougat + pricepercent + sugarpercent + sugarpercent*chocolate,
            data = candy_rankings)
glance(step1)$adj.r.squared
```

```{r}
#remove nougat
step1 <- lm(winpercent ~ chocolate + fruity + pricepercent + sugarpercent + sugarpercent*chocolate +
              pricepercent*fruity, data = candy_rankings)
glance(step1)$adj.r.squared
```

```{r}
#remove pricepercent and its associated interactions
step1 <- lm(winpercent ~ chocolate + fruity + nougat + sugarpercent + 
              sugarpercent*chocolate, data = candy_rankings)
glance(step1)$adj.r.squared
```

```{r}
#remove sugarpercent*chocolate
step1 <- lm(winpercent ~ chocolate + fruity + nougat + sugarpercent + pricepercent +
              pricepercent*fruity, data = candy_rankings)
glance(step1)$adj.r.squared
```

```{r}
#remove pricepercent*fruity
step1 <- lm(winpercent ~ chocolate + fruity + nougat + sugarpercent + pricepercent +
              sugarpercent*chocolate, data = candy_rankings)
glance(step1)$adj.r.squared
```


Of the models from Step 0 and Step 1, the model with the highest adjusted $R^2$ is the one with `nougat` removed. Therefore, we will go to Step 2 eliminating one variable at a time from that model. 

**Step 2: All possible models removing 1 term at a time from the model selected in the previous step**

```{r}
#remove chocolate and its associated interactions
step2 <- lm(winpercent ~ fruity + pricepercent + sugarpercent +
              pricepercent*fruity, data = candy_rankings)
glance(step2)$adj.r.squared
```


```{r}
#remove fruity and its associated interactions
step2 <- lm(winpercent ~ chocolate + pricepercent + sugarpercent + 
              sugarpercent*chocolate,  data = candy_rankings)
glance(step2)$adj.r.squared
```


```{r}
#remove pricepercent and its associated interactions
step2 <- lm(winpercent ~ chocolate + fruity + sugarpercent + 
              sugarpercent*chocolate,  data = candy_rankings)
glance(step2)$adj.r.squared
```


```{r}
#remove sugarpercent*chocolate
step2 <- lm(winpercent ~ chocolate + fruity + sugarpercent + pricepercent +
              pricepercent*fruity,  data = candy_rankings)
glance(step2)$adj.r.squared
```

```{r}
#remove pricepercent*fruity
step2 <- lm(winpercent ~ chocolate + fruity + sugarpercent + pricepercent +
              sugarpercent*chocolate,  data = candy_rankings)
glance(step2)$adj.r.squared
```

The model with `sugarpercent*chocolate` has the highest $R^2$ of all the models we've tested so far, so we will now go to Step 3 eliminating one variable at a time from this model. 
 

**Step 3: All possible models removing 1 term at a time from the model selected in the previous step**

```{r}
#remove chocolate 
step3 <- lm(winpercent ~ fruity + sugarpercent + pricepercent + pricepercent*fruity,  data = candy_rankings)
glance(step3)$adj.r.squared
```

```{r}
#remove fruity and its associated interactions 
step3 <- lm(winpercent ~ chocolate + sugarpercent + pricepercent,  data = candy_rankings)
glance(step3)$adj.r.squared
```

```{r}
#remove sugarpercent
step3 <- lm(winpercent ~ chocolate + fruity + pricepercent +pricepercent*fruity, 
            data = candy_rankings)
glance(step3)$adj.r.squared
```

```{r}
#remove pricepercent and its associated interactions
step3 <- lm(winpercent ~ chocolate + fruity + sugarpercent, 
            data = candy_rankings)
glance(step3)$adj.r.squared
```

```{r}
#remove pricepercent*fruity 
step3 <- lm(winpercent ~ chocolate + fruity + sugarpercent + pricepercent, 
            data = candy_rankings)
glance(step3)$adj.r.squared
```

None of the models in Step 3 resulted in a higher adjusted $R^2$. Therefore our final model is the one selected in Step 2. It is

```{r}
selected_model <- lm(winpercent ~ chocolate + fruity + sugarpercent + pricepercent +
              pricepercent*fruity,  data = candy_rankings)

tidy(selected_model) %>% 
  select(term, estimate) %>% 
  kable(format = "markdown", digits = 3)
glance(selected_model)$adj.r.squared