---
title: "Logistic Regression"
author: "Becky Tang"
date: "07.07.2021"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: "sta199-slides.css"
    logo: img/sta199-sticker-icon.png
    lib_dir: libs/font-awesome
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: "%current%" 
      ratio: "16:9"
---


<!--
layout: true
<div class="my-footer">
<span>
<a href="http://datasciencebox.org" target="_blank">datasciencebox.org</a>
</span>
</div> 
-->


```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warm = 1
  )
# Set dpi and height for images
knitr::opts_chunk$set(fig.height = 3, fig.width = 5, dpi = 300, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.align = "center") 
# ggplot2 color palette with gray
color_palette <- list(gray = "#999999", 
                      salmon = "#E69F00", 
                      lightblue = "#56B4E9", 
                      green = "#009E73", 
                      yellow = "#F0E442", 
                      darkblue = "#0072B2", 
                      red = "#D55E00", 
                      purple = "#CC79A7")
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
# For magick
dev.off <- function(){
  invisible(grDevices::dev.off())
}
# For ggplot2
ggplot2::theme_set(ggplot2::theme_bw())
```

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(broom)
library(knitr)
library(DT)
library(openintro)
library(infer)
library(patchwork)
library(kableExtra)
```


---

class: middle center

## Model Selection

---

## Multiple linear regression

Let's say that we have 3 explanatory variables that we can use to predict the response variable $y$. Should I use all 3? What about interactions?

$$\begin{align} &\hat{y} = \beta_{0} + \beta_{1}x_{1} + \beta_2 x_{2}\\
 &\hat{y} = \beta_{0} + \beta_{1}x_{1} + \beta_2 x_{2} + \beta_{3} x_{3}\\
&\hat{y} = \beta_{0} + \beta_{1}x_{1} + \beta_2 x_{2} + \beta_{3} x_{1}x_{2}\\
& \quad \vdots
\end{align}$$

--

Use Adjusted $R^2$ to help us determine best fitting linear model. 


---

## Backwards elimination

- Start with full model (including all candidate explanatory variables and all candidate interactions)

- Remove one variable at a time, and select the model with the highest adjusted $R^2$

- Continue until adjusted $R^2$ does not increase


---

## Forward elimination

- Start with empty model (only an intercept)

- Add one variable (or interaction effect) at a time, and select the model with the highest adjusted $R^2$

- Continue until adjusted $R^2$ does not increase

--

Note: adjusted $R^2$ is just one criterion; can use other metrics

---
## Backwards elimination

```{r message=FALSE}
sports_car_prices <- read_csv("data/sports_cars.csv")
```

```{r}
m_full <- lm(mileage ~ price + age + car + price*car + age*car, data = sports_car_prices)
```

```{r}
round(glance(m_full)$adj.r.squared, 2)
```

--

Current model: $\widehat{mileage} = \beta_{0} + \beta_{1}~ price + \beta_{2} ~ age + \beta_{3} ~ carPorsche +\beta_{4} ~price*carPorsche + \beta_{5} ~age*carPorsche$ 

Current adjusted $R^2$: 0.77


---

## Backwards elimination

Current model: $\widehat{mileage} = \beta_{0} + \beta_{1}~ price + \beta_{2} ~ age + \beta_{3} ~ carPorsche +\beta_{4} ~price*carPorsche + \beta_{5} ~age*carPorsche$ 

Current adjusted $R^2$: 0.77

```{r}
m_cand1 <- lm(mileage ~ price + age + car + price*car, data = sports_car_prices)
m_cand2 <- lm(mileage ~ price + age + car + age*car, data = sports_car_prices)

round(glance(m_cand1)$adj.r.squared, 2)
round(glance(m_cand2)$adj.r.squared, 2)
```
--
New model: $\widehat{mileage} = \beta_{0} + \beta_{1}~ price + \beta_{2} ~ age + \beta_{3} ~ carPorsche + \beta_{4} ~age*carPorsche$

New adjusted $R^2$: 0.77

---

## Backwards elimination

Current model: $\widehat{mileage} = \beta_{0} + \beta_{1}~ price + \beta_{2} ~ age + \beta_{3} ~ carPorsche + \beta_{4} ~age*carPorsche$

Current adjusted $R^2$: 0.77


```{r}
m_cand1 <- lm(mileage ~ price + age + car, data = sports_car_prices)
m_cand2 <- lm(mileage ~ age + car + age*car, data = sports_car_prices)
round(glance(m_cand1)$adj.r.squared, 2)
round(glance(m_cand2)$adj.r.squared, 2)
```

--

New model: $\widehat{mileage} = \beta_{0} + \beta_{1}~ price + \beta_{2} ~ age + \beta_{3} ~ carPorsche$

New adjusted $R^2$: 0.77

---

## Backwards elimination

Current model: $\widehat{mileage} = \beta_{0} + \beta_{1}~ price + \beta_{2} ~ age + \beta_{3} ~ carPorsche$

Current adjusted $R^2$: 0.77


```{r}
m_cand1 <- lm(mileage ~ price + age, data = sports_car_prices)
m_cand2 <- lm(mileage ~ age + car, data = sports_car_prices)
m_cand3 <- lm(mileage ~ price + car, data = sports_car_prices)

round(glance(m_cand1)$adj.r.squared, 2)
round(glance(m_cand2)$adj.r.squared, 2)
round(glance(m_cand3)$adj.r.squared, 2)
```

---

## Backwards elimination

Adjusted $R^2$ stopped increasing, so we have our final model.

```{r}
m_final <- lm(mileage ~ price + age + car, data = sports_car_prices)
tidy(m_final)
```

$\widehat{mileage} = 48960.15 -0.73 ~ price + 2100.21 ~ age + 10034.86 ~ carPorsche$

---

## `step()` function

- Automate the process of model selection using the `step` function.

- Note: the criterion the `step()` uses to compare models is Akaike information criterion (AIC)

```{r, eval = F}
m_final <- step(m_full, direction = "backward")
```

---

## `step()` function

```{r echo = F}
m_final <- step(m_full, direction = "backward")
```

---


## `step()` function

```{r}
tidy(m_final)
```

---

class: middle center

## Logistic Regression

---

## Introduction

Multiple regression allows us to relate a numerical response variable to one or
more numerical or categorical predictors. 

We can use multiple regression models
to understand relationships, assess differences, and make predictions.

But what about a situation where the response of interest is categorical and
binary?

--

- spam or not spam
- malignant or benign tumor
- survived or died
- admitted or or not admitted

---


## Titanic

On April 15, 1912 the famous ocean liner *Titanic* sank in the North Atlantic 
after striking an iceberg on its maiden voyage. The dataset `titanic.csv` 
contains the survival status and other attributes of individuals on the titanic.

- `survived`: survival status  (1 = survived, 0 = died)
- `pclass`: passenger class (1 = 1st, 2 = 2nd, 3 = 3rd)
- `name`: name of individual
- `sex`: sex (male or female)
- `age`: age in years
- `fare`: passenger fare in British pounds

We are interested in investigating the variables that contribute to passenger 
survival. Do women and children really come first? 

---

## Data and Packages

```{r load-packages, eval = FALSE}
library(tidyverse)
library(broom)
```

```{r load-data, message=FALSE, echo = F}
titanic <- read_csv("data/titanic.csv") %>%
  mutate(survived = if_else(died == 0,1,0))
```

```{r glimpse-data}
glimpse(titanic)
```

---

## Exploratory Data Analysis

```{r eda-1, echo=FALSE}
p1 <- titanic %>%
  mutate(Survival = if_else(died == 1, "Died", "Survived")) %>%
  ggplot(aes(x = sex, fill = Survival)) +
  geom_bar(position = "fill") +
  labs(x = "Sex", y = "")
p2 <- titanic %>%
  mutate(Survival = if_else(died == 1, "Died", "Survived")) %>%
  ggplot(aes(x = Survival, y = age)) +
  geom_boxplot() +
  labs(y = "Age")
```


```{r echo = F, fig.height = 2.5}
p1 + p2
```



---



## The linear model with multiple predictors

- Population model:

$$ y = \beta_0 + \beta_1~x_1 + \beta_2~x_2 + \cdots + \beta_k~x_k + \epsilon $$

--

- Sample model that we use to estimate the population model:
  
$$ \hat{y} = b_0 + b_1~x_1 + b_2~x_2 + \cdots + b_k~x_k $$

--

Denote by $p$ the probability of survival and consider the model below.

$$ p = \beta_0 + \beta_1~x_1 + \beta_2~x_2 + \cdots + \beta_k~x_k + \epsilon$$


--


Can you see any problems with this approach?

---

## Linear Regression?


```{r linear-model}
lm_survival <- lm(survived ~ age + sex, data = titanic)
tidy(lm_survival)
```



---

## Visualizing the Model

```{r linear-model-viz, echo = FALSE, fig.height = 2}
ggplot(
  titanic,
  aes(x = age, y = survived, color = sex)
) +
  geom_jitter(width = 0.05, height = .05, alpha = .5) +
  geom_abline(intercept = 0.248, slope = 0.000343, color = "#F57670", lwd = 1) +
  geom_abline(intercept = 0.799, slope = 0.000343, color = "#1FBEC3", lwd = 1)
```

---

## Diagnostics

```{r linear-model-diag-1, echo = FALSE, fig.height = 2.5}
lm_survival_aug <- augment(lm_survival)
p1 <- ggplot(
  data = lm_survival_aug,
  aes(x = .fitted, y = .resid)
) +
  geom_point() +
  labs(x = "Predicted", y = "Residuals") +
  geom_hline(yintercept = 0, lwd = 2, lty = 2, col = "red")
p2 <- ggplot(
  lm_survival_aug,
  aes(x = .resid, y = .fitted)
) +
  geom_point() +
  geom_hline(yintercept = c(0, 1), lwd = 2, lty = 2, col = "red") +
  labs(x = "Age", y = "Predicted")
```

```{r echo = F, fig.height = 2}
p1 + p2
```


--

This isn't helpful! We need to develop a new tool.


---

## Preliminaries

- Denote by $p$ the probability of some event
- The .vocab[odds] the event occurs is $$\frac{p}{1-p}$$

--

Odds are sometimes expressed as X : Y and read X to Y. 

It is the ratio of successes to 
failures, where values larger than 1 favor a success and values smaller than 1
favor a failure.

--

If $P(A) = 1/2$, the odds of $A$ are $\color{#1689AE}{\frac{1/2}{1/2} = 1}$

--

If $P(B) = 1/3$, the odds of $B$ are $\color{#1689AE}{\frac{1/3}{2/3} = 0.5}$ 

--

If $P(B) = 2/3$, the odds of $B$ are $\color{#1689AE}{\frac{2/3}{1/3} = 2}$ 

--


An .vocab[odds ratio] is a ratio of odds.

---

## Preliminaries

- Taking the natural log of the odds yields the .vocab[logit] of $p$

$$\text{logit}(p) = \text{log}\left(\frac{p}{1-p}\right)$$

--

The logit takes a value of $p$ between 0 and 1 and outputs a value between 
$-\infty$ and $\infty$.

--

The .vocab[inverse logit (expit)] takes a value between $-\infty$ and $\infty$ and 
outputs a value between 0 and 1.

$$\text{inverse logit}(x) = \frac{e^x}{1+e^x}$$

---

## Logistic Regression Model

.alert[
$$\text{log}\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_k x_{k}$$
]

--

Use the inverse logit to find the expression for $p$.

.alert[
$$p = \frac{e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_k x_{k}}}{1 + e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_k x_{k}}}$$
]

We can use the logistic regression model to obtain predicted probabilities of 
success for a binary response variable.

---

## Logistic Regression Model

We handle fitting the model via computer using the `glm` function.

```{r fit-logistic}
logit_mod <- glm(survived ~ sex + age, data = titanic, 
                 family = "binomial")
tidy(logit_mod)
```

---

## Logistic Regression Model

And use `augment` to find predicted log-odds.

```{r augment-logistic}
pred_log_odds <- augment(logit_mod)
```


---

## The Estimated Logistic Regression Model

.midi[
```{r tidy-model}
tidy(logit_mod)
```
]

$$\text{log}\left(\frac{\hat{p}}{1-\hat{p}}\right) = 1.11 - 2.50~sex - 0.00206~age$$
$$\hat{p} = \frac{e^{1.11 - 2.50~sex - 0.00206~age}}{{1+e^{1.11 - 2.50~sex - 0.00206~age}}}$$

---

## Interpreting coefficients

.alert[
$$\text{log}\left(\frac{\hat{p}}{1-\hat{p}}\right) = 1.11 - 2.50~sex - 0.00206~age$$
]

<br> 

--

Holding sex constant, for every additional year of age, we expect the **log-odds** of survival to decrease by approximately 0.002.

<br> 

--

Holding age constant, we expect males to have a **log-odds** of survival that is 2.50 less than females.

---

## Interpreting coefficients

.alert[
$$\frac{\hat{p}}{1-\hat{p}} = e^{1.11 - 2.50~sex - 0.00206~age}$$
]

<br> 

--
$$e^{x+1} = e^{x} \times e^{1} \Rightarrow e^{1.11 - 2.50sex -0.00206(age + 1)} = e^{1.11 - 2.50sex -0.00206age} \times e^{-0.0026}$$
--

Holding sex constant, for every one year increase in age, the **odds** of survival are expected to multiply by a factor of $e^{-0.00206} = 0.998$. 

<br>

--

Holding age constant, the **odds** of survival for males are $e^{-2.50} = 0.082$ times the odds of survival for females.

---

## Classification

- Logistic regression allows us to obtain predicted probabilities of success
for a binary variable.
- By imposing a threshold (for example if the probability is greater than 
$0.50$) we can create a classifier.

--

Observed vs predicted survival:

```{r echo = F}
log_aug <- augment(logit_mod, type.predict = "response")
log_aug %>%
  mutate(pred_survival = if_else(.fitted > 0.5, "Survived", "Died")) %>%
  count(survived, pred_survival) %>%
  pivot_wider(names_from = pred_survival, 
              values_from = n
              ) 
```

---

## Strengths and Weaknesses 

.vocab[Weaknesses]

- Logistic regression has assumptions: independence and linearity in the log-odds (some other methods require fewer assumptions)
- If the predictors are correlated,  coefficient estimates may be unreliable

--

.vocab[Strengths]

- Straightforward interpretation of coefficients
- Handles numerical and categorical predictors
- Can quantify uncertainty around a prediction
- Can extend to more than 2 categories (multinomial regression)
