<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Prediction + Model validation</title>
    <meta charset="utf-8" />
    <meta name="author" content="Dr. Maria Tackett" />
    <link href="libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <script src="libs/htmlwidgets/htmlwidgets.js"></script>
    <script src="libs/jquery/jquery.min.js"></script>
    <link href="libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
    <script src="libs/datatables-binding/datatables.js"></script>
    <link href="libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
    <link href="libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
    <script src="libs/dt-core/js/jquery.dataTables.min.js"></script>
    <link href="libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
    <script src="libs/crosstalk/js/crosstalk.min.js"></script>
    <link rel="stylesheet" href="sta199-slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Prediction + Model validation
### Dr. Maria Tackett
### 10.24.19

---


layout: true

&lt;div class="my-footer"&gt;
&lt;span&gt;
&lt;a href="http://datasciencebox.org" target="_blank"&gt;datasciencebox.org&lt;/a&gt;
&lt;/span&gt;
&lt;/div&gt; 

---






class: middle, center

### [Click for PDF of slides](08c-predict-validate.pdf)

---

### Announcements

- Peer Feedback #2 **due TODAY at 11:59p**

- Complete [Reading 06](https://www2.stat.duke.edu/courses/Fall19/sta199.001/reading/reading-06.html) if you haven't already

- Project proposal **due Friday at 11:59p**

- HW 03 **due Thursday, Oct 31 at 11:59p**

---

class: center, middle

# Model selection

---

## Data: Candy Rankings

- Take about 10 - 15 minutes to finish/ review your model selection and data visualization 

- Make sure all your work is in one RStudio Cloud project 
    - Call that project Ultimate Candy Rankings - Model Selection 
---

### Packages


```r
library(tidyverse)
library(broom)
library(knitr)
library(modelr)  # new!
```

---

## Full model

.question[
What percent of the variability in win percentages is explained by the model?
]


```r
full_model &lt;- lm(winpercent ~ chocolate + fruity + caramel + peanutyalmondy +
                   nougat + crispedricewafer + hard + bar + 
                   pluribus + sugarpercent + pricepercent, 
                 data = candy_rankings)
glance(full_model)$r.squared
```

```
## [1] 0.5402088
```

```r
glance(full_model)$adj.r.squared
```

```
## [1] 0.4709252
```

---

## Akaike Information Criterion

$$ AIC = -2log(L) + 2k $$

- `\(L\)`: likelihood	of the	model
    - Likelihood of seeing these data	given	the estimated model parameters
    - Won't go into calculating it in this course (but you will in future courses)
    
--

- Used for model selection, lower the better
    - Value is not informative on its own
--

- Applies	a	penalty	for	number of parameters in the	model, `\(k\)`
    - Different penalty than adjusted `\(R^2\)` but similar idea


```r
glance(full_model)$AIC
```

```
## [1] 657.2701
```

---

## Model selection -- a little faster


```r
selected_model &lt;- step(full_model, direction = "backward")
```


```r
tidy(selected_model) %&gt;% select(term, estimate) %&gt;% 
  kable(format = "markdown", digits = 3)
```



|term                 | estimate|
|:--------------------|--------:|
|(Intercept)          |   32.941|
|chocolateTRUE        |   19.147|
|fruityTRUE           |    8.881|
|peanutyalmondyTRUE   |    9.483|
|crispedricewaferTRUE |    8.385|
|hardTRUE             |   -5.669|
|sugarpercent         |    7.979|

---

## Selected variables

| variable     | selected    |
| ------------ | :----------:|
| chocolate         |x            |
| fruity   | x           |
| caramel     |            |
| peanutyalmondy     | x           |
| nougat          |            |
| crispedricewafer| x           |
| hard |x            |
| bar |             |
| pluribus    |             |
| sugarpercent    |x            |
| pricepercent  |           |

---

## Coefficient interpretation

.question[
Interpret the slopes of `chocolate` and `sugarpercent` in context of the data.
]


|term                 | estimate|
|:--------------------|--------:|
|(Intercept)          |   32.941|
|chocolateTRUE        |   19.147|
|fruityTRUE           |    8.881|
|peanutyalmondyTRUE   |    9.483|
|crispedricewaferTRUE |    8.385|
|hardTRUE             |   -5.669|
|sugarpercent         |    7.979|

---

## AIC

As expected, the selected model has a smaller AIC than the full model. In fact, the selected model has the minimum AIC of all possible main effects models. 


```r
glance(full_model)$AIC
```

```
## [1] 657.2701
```


```r
glance(selected_model)$AIC
```

```
## [1] 649.5113
```

---

## Parismony

.pull-left[
.question[
Look at the variables in the full and the selected model. Can you guess
why some of them may have been dropped? Remember: We like parsimonious models.
]
]


.pull-right[
.midi[
| variable     | selected    |
| ------------ | :----------:|
| chocolate         |x            |
| fruity   | x           |
| caramel     |            |
| peanutyalmondy     | x           |
| nougat          |            |
| crispedricewafer| x           |
| hard |x            |
| bar |             |
| pluribus    |             |
| sugarpercent    |x            |
| pricepercent  |           |

]
]

---


class: center, middle

# Prediction

---

### New observation

To make a prediction for a new observation we need to create a data frame with that observation.

&lt;div class="question"&gt;
Suppose we want to make a prediction for a candy that contains chocolate, isn't fruity, has no peanuts or almonds, has a wafer, isn't hard, and has a sugar content in the 20th percentile.
&lt;br&gt;&lt;br&gt;
The following will result in an incorrect prediction. Why? How would you correct it?
&lt;/div&gt;


```r
new_candy &lt;- tibble(chocolate = TRUE, 
                       fruity = FALSE, 
                       peanutyalmondy = FALSE, 
                       crispedricewafer = TRUE,
                       hard = FALSE, 
                       sugarpercent = 20)
```

---

### New observation, corrected


```r
new_candy &lt;- tibble(chocolate = TRUE, 
                       fruity = FALSE, 
                       peanutyalmondy = FALSE, 
                       crispedricewafer = TRUE,
                       hard = FALSE, 
                       sugarpercent = 0.20)
```

---

### Prediction


```r
predict(selected_model, newdata = new_candy)
```

```
##        1 
## 62.06853
```

---

### Uncertainty around prediction

- Confidence interval around `\(\hat{y}\)` for new data (average win percentage for candy types with the given characteristics):


```r
predict(selected_model, newdata = new_candy, interval = "confidence")
```

```
##        fit      lwr      upr
## 1 62.06853 53.65186 70.48519
```

- Prediction interval around `\(\hat{y}\)` for new data (predicted score for an individual type of candy with the given characteristics ):


```r
predict(selected_model, newdata = new_candy, interval = "prediction")
```

```
##        fit      lwr      upr
## 1 62.06853 39.54943 84.58762
```

---

class: center, middle

# Model validation

**(optional, supplementary material)**

---

### Overfitting

- The data we are using to construct our models come from a larger population.

- Ultimately we want our model to tell us how the population works, not just the sample we have.

- If the model we fit is too tailored to our sample, it might not perform as well with the remaining population. This means the model is "overfitting" our data.

- We measure this using .vocab[model validation] techniques.

- Note: Overfitting is not a huge concern with linear models with low level interactions, however it can be with more complex and flexible models. The following is just an example of model validation, even though we're using it in a scenario where the concern for overfitting is not high.

---

### Model validation

- One commonly used model validation technique is to partition your data into training
and testing set

- That is, fit the model on the training data

- And test it on the testing data

- Evaluate model performance using `\(RMSE\)`, root-mean squared error

$$ RMSE = \sqrt{\frac{\sum_{i = 1}^n (y_i - \hat{y}_i)^2}{n}} $$

.question[
Do you think we should prefer low or high RMSE?
]

---

### Random sampling and reproducibility

Gotta set a seed!

```r
set.seed(102319)
```

- Use different seeds from each other

- Need inspiration? https://www.random.org/

---

### Cross validation

More specifically, &lt;font class="vocab"&gt;k-fold cross validation&lt;/font&gt;

- Split your data into k folds.

- Use 1 fold for testing and the remaining (k - 1) folds for training.

- Repeat k times.

---

### Aside -- the modulo operator


```r
9 %% 5
```

```
## [1] 4
```

--

.pull-left[
<div id="htmlwidget-a0b0f6d146e676cca251" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-a0b0f6d146e676cca251">{"x":{"filter":"none","data":[[1,2,3,4,5,6,7,8],[1,2,3,4,5,1,2,3]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>obs<\/th>\n      <th>fold<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"t","columnDefs":[{"className":"dt-right","targets":[0,1]}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
]

--

.pull-right[

```r
(1:8) %% 5
```

```
## [1] 1 2 3 4 0 1 2 3
```

```r
((1:8) - 1) %% 5
```

```
## [1] 0 1 2 3 4 0 1 2
```

```r
(((1:8) - 1) %% 5) + 1
```

```
## [1] 1 2 3 4 5 1 2 3
```
]

---

### Prepping your data for 5-fold CV


```r
candy_rankings_cv &lt;- candy_rankings %&gt;%
  mutate(id = 1:n()) %&gt;%
  sample_n(nrow(candy_rankings)) %&gt;%
  mutate( fold = (((1:n()) - 1) %% 5) + 1 )

candy_rankings_cv %&gt;% 
  count(fold)
```

```
## # A tibble: 5 x 2
##    fold     n
##   &lt;dbl&gt; &lt;int&gt;
## 1     1    17
## 2     2    17
## 3     3    17
## 4     4    17
## 5     5    17
```

---

### CV 1


```r
test_fold &lt;- 1
test &lt;- candy_rankings_cv %&gt;% filter(fold == test_fold)
train &lt;- candy_rankings_cv %&gt;% anti_join(test, by = "id")
mod &lt;- lm(winpercent ~ chocolate + fruity + peanutyalmondy + crispedricewafer + hard + sugarpercent, data = candy_rankings)
(rmse_test1 &lt;- rmse(mod, test))
```

```
## [1] 10.2658
```

---

### RMSE on training vs. testing

.question[
Would you expect the RMSE to be higher for your training data or your testing data? Why?
]

---

### RMSE on training vs. testing

RMSE for testing:
.small[

```r
(rmse_test1 &lt;- rmse(mod, test))
```

```
## [1] 10.2658
```
]

RMSE for training:
.small[

```r
(rmse_train1 &lt;- rmse(mod, train))
```

```
## [1] 9.995652
```
]

---

### CV 2


```r
test_fold &lt;- 2
test &lt;- candy_rankings_cv %&gt;% filter(fold == test_fold)
train &lt;- candy_rankings_cv %&gt;% anti_join(test, by = "id")
mod &lt;- lm(winpercent ~ chocolate + fruity + peanutyalmondy + crispedricewafer + hard + sugarpercent, data = candy_rankings)
```


```r
(rmse_test2 &lt;- rmse(mod, test))
```

```
## [1] 9.106138
```

```r
(rmse_train2 &lt;- rmse(mod, train))
```

```
## [1] 10.27274
```

---

### CV 3


```r
test_fold &lt;- 3
test &lt;- candy_rankings_cv %&gt;% filter(fold == test_fold)
train &lt;- candy_rankings_cv %&gt;% anti_join(test, by = "id")
mod &lt;- lm(winpercent ~ chocolate + fruity + peanutyalmondy + crispedricewafer + hard + sugarpercent, data = candy_rankings)
```


```r
(rmse_test3 &lt;- rmse(mod, test))
```

```
## [1] 10.54619
```

```r
(rmse_train3 &lt;- rmse(mod, train))
```

```
## [1] 9.922409
```

---

### CV 4


```r
test_fold &lt;- 4
test &lt;- candy_rankings_cv %&gt;% filter(fold == test_fold)
train &lt;- candy_rankings_cv %&gt;% anti_join(test, by = "id")
mod &lt;- lm(winpercent ~ chocolate + fruity + peanutyalmondy + crispedricewafer + hard + sugarpercent, data = candy_rankings)
```


```r
(rmse_test4 &lt;- rmse(mod, test))
```

```
## [1] 10.16521
```

```r
(rmse_train4 &lt;- rmse(mod, train))
```

```
## [1] 10.02132
```

---

### CV 5


```r
test_fold &lt;- 5
test &lt;- candy_rankings_cv %&gt;% filter(fold == test_fold)
train &lt;- candy_rankings_cv %&gt;% anti_join(test, by = "id")
mod &lt;- lm(winpercent ~ chocolate + fruity + peanutyalmondy + crispedricewafer + hard + sugarpercent, data = candy_rankings)
```


```r
(rmse_test5 &lt;- rmse(mod, test))
```

```
## [1] 10.10826
```

```r
(rmse_train5 &lt;- rmse(mod, train))
```

```
## [1] 10.03571
```

---

### Putting it altogether

.small[

```r
rmse_candy &lt;- tibble(
  test_fold  = 1:5,
  rmse_train = c(rmse_train1, rmse_train2, rmse_train3, rmse_train4, rmse_train5),
  rmse_test  = c(rmse_test1, rmse_test2, rmse_test3, rmse_test4, rmse_test5)
)
```


```r
ggplot(data = rmse_candy, mapping = aes(x = test_fold, y = rmse_test)) +
  geom_point() +
  geom_line()
```

![](08c-predict-validate_files/figure-html/unnamed-chunk-30-1.png)&lt;!-- --&gt;
]

---

### How does RMSE compare to y?

- `winpercent` summary stats:


```
## # A tibble: 1 x 6
##     min   max  mean   med    sd   IQR
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  22.4  84.2  50.3  47.8  14.7  20.7
```

- `rmse_test` summary stats:


```
## # A tibble: 1 x 6
##     min   max  mean   med    sd   IQR
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  9.11  10.5  10.0  10.2 0.548 0.158
```

---

### `modelr` in tidyverse

.pull-left[
![](img/08c/modelr-part-of-tidyverse.png)
]
.pull-right[
The **modelr** package provides functions that help you create pipelines when modeling.

```r
library(modelr)
```
.footnote[
[modelr.tidyverse.org](https://modelr.tidyverse.org)
]
]



---

### Cross Validation - Faster

- **`modelr::crossv_kfold`**: Partition data into k folds

- **`purrr::map`**: Fit a model on each of the training sets

- Calculate RMSEs for each of the models on the testing set

---

### Partition data into k folds.

k = 5:


```r
candy_rankings_cv &lt;- crossv_kfold(candy_rankings, 5)
candy_rankings_cv
```

```
## # A tibble: 5 x 3
##   train        test         .id  
##   &lt;named list&gt; &lt;named list&gt; &lt;chr&gt;
## 1 &lt;resample&gt;   &lt;resample&gt;   1    
## 2 &lt;resample&gt;   &lt;resample&gt;   2    
## 3 &lt;resample&gt;   &lt;resample&gt;   3    
## 4 &lt;resample&gt;   &lt;resample&gt;   4    
## 5 &lt;resample&gt;   &lt;resample&gt;   5
```

---

### Fit model on each of training set


```r
models &lt;- map(candy_rankings_cv$train, ~ 
              lm(winpercent ~ chocolate + fruity + peanutyalmondy +
                   crispedricewafer + hard + sugarpercent,
                 data =.))
```

---

### Calculate RMSEs

.question[
Explain how `map2_dbl` works.
]


```r
rmses &lt;- map2_dbl(models, candy_rankings_cv$test, rmse)
rmses
```

```
##         1         2         3         4         5 
## 10.877690  9.373646 10.881654  7.517380 13.240856
```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true,
"highlightStyle": "github",
"countIncrementalSlides": false,
"slideNumberFormat": "%current%"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
