---
title: "Formalizing Linear Models"
author: "Dr. Maria Tackett"
date: "10.15.19"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: "sta199-slides.css"
    logo: img/sta199-sticker-icon.png
    lib_dir: libs
    nature: 
      highlightLines: true
      highlightStyle: github
      countIncrementalSlides: false
      slideNumberFormat: "%current%"
---

layout: true

<div class="my-footer">
<span>
<a href="http://datasciencebox.org" target="_blank">datasciencebox.org</a>
</span>
</div> 

---

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
library(emo)
library(DT)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning=FALSE,
                      message=FALSE,
                      fig.height = 2.65, 
                      dpi = 300) 
# For nonsese...
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
# For magick
dev.off <- function(){
  invisible(grDevices::dev.off())
}
```

class: middle, center

### [Click for PDF of slides](07b-formalizing-linear-models.pdf)

---

## Announcements

- Complete [Reading 05](https://www2.stat.duke.edu/courses/Fall19/sta199.001/reading/reading-05.html) (if you haven't already done so)

- Project topic ideas **due Wednesday at 11:59p**

---

class: center, middle

## Characterizing relationships with models

---

## Data & packages

```{r message=FALSE}
library(tidyverse)
library(broom)
```

```{r message=FALSE}
pp <- read_csv("data/paris_paintings.csv", 
               na = c("n/a", "", "NA"))
```

---

## Want to follow along?

Go to RStudio Cloud -> make a copy of "Modeling Paris Paintings"

---

## Height & width

```{r}
(m_ht_wt <- lm(Height_in ~ Width_in, data = pp))
```

--

<br>

$$\widehat{Height_{in}} = 3.62 + 0.78~Width_{in}$$

--

- .vocab[Slope]: For each additional inch the painting is wider, the height is expected
to be higher, on average, by 0.78 inches.

--

- .vocab[Intercept]: Paintings that are 0 inches wide are expected to be 3.62 inches high,
on average.
    - This is a nonsense interpretation!

---

### The linear model with a single predictor

- We're interested in the $\beta_0$ (population parameter for the intercept)
and the $\beta_1$ (population parameter for the slope) in the 
following model:

$$ \hat{y} = \beta_0 + \beta_1~x $$

--

- Tough luck, you can't have them...

--

- So we use the sample statistics to estimate them:

$$ \hat{y} = b_0 + b_1~x $$

---

## Least squares regression

The regression line minimizes the sum of squared residuals.

--

If $e_i = y - \hat{y}$,

then, the regression line minimizes $\sum_{i = 1}^n e_i^2$.

---

## Visualizing residuals

```{r echo=FALSE}
d <- tibble(
    Width_in     = m_ht_wt$model$Width_in,
    Height_in    = m_ht_wt$model$Height_in,
    pred         = m_ht_wt$fitted.values,
    res          = m_ht_wt$residuals
  )
p <- ggplot(data = d, mapping = aes(x = Width_in, y = Height_in)) +
  geom_point(alpha = 0.2) + 
  theme_bw() +
  labs(title = "Height vs. width of paintings", subtitle = "Just the data") +
  xlim(0, 250) +
  ylim(0, 200)
p
```

---

## Visualizing residuals (cont.)

```{r echo=FALSE}
p <- p + 
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(mapping = aes(y = pred)) +
  labs(subtitle = "Data + least squares resgression line")
p
```

---

## Visualizing residuals (cont.)

```{r echo = FALSE}
p + 
  geom_segment(mapping = aes(xend = Width_in, yend = pred), color="red", alpha = 0.4) +
  labs(subtitle = "Data + least squares resgression line + residuals")
```


---

### Properties of the least squares regression line

- The slope has the same sign as the correlation coefficient:

$$b_1 = r \frac{s_y}{s_x}$$

- The regression line goes through the center of mass point, the coordinates corresponding to average $x$ and average $y$: $(\bar{x}, \bar{y})$.

$$\hat{y} = b_0 + b_1 x \hspace{5mm} ~ \Rightarrow  \hspace{5mm}~ b_0 = \bar{y} - b_1 \bar{x}$$

---

### Properties of the least squares regression line

- The sum of the residuals is zero: 

$$\sum_{i = 1}^n e_i = 0$$

- The residuals and $x$ values are uncorrelated.

---

### Height & landscape features

```{r}
(m_ht_lands <- lm(Height_in ~ factor(landsALL), data = pp))
```

--

<br>

$$\widehat{Height_{in}} = 22.68 - 5.65~landsALL$$

---

### Height & landscape features (cont.)

- .vocab[Slope]: Paintings with landscape features are expected, on average,
to be 5.65 inches shorter than paintings that without landscape features.
    - Compares baseline level (`landsALL = 0`) to other level
    (`landsALL = 1`).

- .vocab[Intercept]: Paintings that don't have landscape features are expected, on 
average, to be 22.68 inches tall.

---

### Categorical predictor with 2 levels

```{r echo=FALSE}
pp %>% 
  select(name, price, landsALL) %>% 
  slice(1:8)
```

---

### Relationship between height and school

```{r}
(m_ht_sch <- lm(Height_in ~ school_pntg, data = pp))
```

--

- When the categorical explanatory variable has many levels, they're encoded to
<font class="vocab">dummy (indicator) variables</font>.

- Each coefficient describes the expected difference between heights in that 
particular school compared to the baseline level.

---

### Categorical predictor with >2 levels

.small[
```{r echo=FALSE}
pp %>% 
  select(school_pntg) %>% 
  group_by(school_pntg) %>% 
  sample_n(1) %>%
  mutate(
    D_FL = as.integer(ifelse(school_pntg == "D/FL", 1L, 0)),
    F    = as.integer(ifelse(school_pntg == "F", 1L, 0)),
    G    = as.integer(ifelse(school_pntg == "G", 1L, 0)),
    I    = as.integer(ifelse(school_pntg == "I", 1L, 0)),
    S    = as.integer(ifelse(school_pntg == "S", 1L, 0)),
    X    = as.integer(ifelse(school_pntg == "X", 1L, 0))
  ) %>%
  datatable()
```
]

---

### Relationship between height and school 

```{r echo = F}
(m_ht_sch <- lm(Height_in ~ school_pntg, data = pp))
```


.question[
1. What is the expected height of paintings from School A?

2. Interpret the slope for `school_pntgG`.

3. What is the expected height of paintings from School I?
]


---

### Correlation does not imply causation!

Remember this when interpreting model coefficients

---

class: center, middle

# Prediction with models

---

## Predict height from width

.question[
On average, how tall are paintings that are 60 inches wide?
$$\widehat{Height_{in}} = 3.62 + 0.78~Width_{in}$$
]

--

```{r}
3.62 + 0.78 * 60
```

"On average, we expect paintings that are 60 inches wide to be 50.42 inches high."

**Warning:** We "expect" this to happen, but there will be some variability. (We'll
learn about measuring the variability around the prediction later.)

---

## Prediction vs. extrapolation

.question[
On average, how tall are paintings that are 400 inches wide?
$$\widehat{Height_{in}} = 3.62 + 0.78~Width_{in}$$
]

```{r extrapolate, warning = FALSE, echo=FALSE, fig.height=2.75}
newdata <- tibble(Width_in = 400)
newdata <- newdata %>%
  mutate(Height_in = predict(m_ht_wt, newdata = newdata))

ggplot(data = pp, aes(x = Width_in, y = Height_in)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", fullrange = TRUE, se = FALSE) +
  xlim(0, 420) +
  ylim(0, 320) +
  geom_segment(data = newdata, mapping = aes(x = Width_in, y = 0, xend = Width_in, yend = Height_in), color = "red", lty = 2) +
  geom_segment(data = newdata, mapping = aes(x = Width_in, y = Height_in, xend = 0, yend = Height_in), color = "red", lty = 2)
```

---

## Watch out for extrapolation!

> "When those blizzards hit the East Coast this winter, it proved to my satisfaction 
that global warming was a fraud. That snow was freezing cold. But in an alarming 
trend, temperatures this spring have risen. Consider this: On February 6th it was 10 
degrees. Today it hit almost 80. At this rate, by August it will be 220 degrees. So 
clearly folks the climate debate rages on."<sup>1</sup>  <br>
Stephen Colbert, April 6th, 2010

.footnote[
[1] OpenIntro Statistics. "Extrapolation is treacherous." OpenIntro Statistics.
]

---

class: center, middle

# Measuring model fit

---

## Measuring the strength of the fit

- The strength of the fit of a linear model is most commonly evaluated using $R^2$.

- It tells us what percent of variability in the response variable is explained by 
the model.

- The remainder of the variability is explained by variables not included in the 
model.

- $R^2$ is sometimes called the coefficient of determination.

---

## Obtaining $R^2$ in R

- Height vs. width
.small[
```{r}
glance(m_ht_wt)
glance(m_ht_wt)$r.squared # extract R-squared
```
]

Roughly 68% of the variability in heights of paintings can be explained by their
widths.

---
class: center, middle

# Tidy regression output

---

class: middle

Let's revisit the model predicting heights of paintings from their widths:

```{r}
m_ht_wt <- lm(Height_in ~ Width_in, data = pp)
```

---

## Not-so-tidy regression output

- You might come across these as you read work from others, but we'll try to stay away from them

- Not because they are wrong, but because they don't result in tidy data frames as results.

---

## Not-so-tidy regression output (1)

Option 1:

```{r}
m_ht_wt
```

---

## Not-so-tidy regression output (2)

Option 2:

```{r}
summary(m_ht_wt)
```

---

## Review

.question[
What makes a data frame tidy?
]

--

1. Each variable forms a column.
2. Each observation forms a row.
3. Each type of observational unit forms a table.

---

## Tidy regression output

Achieved with functions from the broom package:

- **`tidy`**: Constructs a data frame that summarizes the model's statistical findings: coefficient estimates, *standard errors, test statistics, p-values*.

- **`augment`**: Adds columns to the original data that was modeled. This includes predictions and residuals.

- **`glance`**: Constructs a concise one-row summary of the model. This typically contains values such as $R^2$, adjusted $R^2$, *and residual standard error that are computed once for the entire model*.

---

### Tidy your model's statistical findings

```{r}
tidy(m_ht_wt)
```

--

```{r}
tidy(m_ht_wt) %>%
  select(term, estimate) 
```

---

### Augment data with model results

New variables of note (for now):

- **`.fitted`**: Predicted value of the response variable
- **`.resid`**: Residuals

.small[
```{r}
augment(m_ht_wt) %>%
  slice(1:5)
```
]

--

.question[
Why might we be interested in these new variables?
]

---

## Residuals plot

.small[
```{r fig.height=2}
m_ht_wt_aug <- augment(m_ht_wt)
ggplot(m_ht_wt_aug, mapping = aes(x = .fitted, y = .resid)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "blue", lty = 2) +
  labs(x = "Predicted height", y = "Residuals")
```
]

--

.question[
What does this plot tell us about the fit of the linear model?
]

---

## Glance to assess model fit

```{r}
glance(m_ht_wt)
```

--

```{r}
glance(m_ht_wt)$r.squared
```

--

The $R^2$ is `r round(glance(m_ht_wt)$r.squared * 100, 2)`%.

