<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Data science ethics</title>
    <meta charset="utf-8" />
    <meta name="author" content="Becky Tang" />
    <script src="libs/font-awesome/header-attrs/header-attrs.js"></script>
    <link href="libs/font-awesome/font-awesome/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <link rel="stylesheet" href="math118-slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Data science ethics
]
.author[
### Becky Tang
]
.date[
### 12/1/2022
]

---


layout: true

&lt;div class="my-footer"&gt;
&lt;span&gt;
&lt;a href="http://datasciencebox.org" target="_blank"&gt;datasciencebox.org&lt;/a&gt;
&lt;/span&gt;
&lt;/div&gt; 

---






## Topics

 - Misrepresenting data

 - Algorithmic bias

 - Privacy?
 
 - Simpson's paradox

---

class: middle, center

## Misrepresenting data

---

.question[
What is the difference between these two pictures? Which presents a better way to represent these data?
]

&lt;br&gt;

&lt;img src="img/09/axis-start-at-0.png" width="80%" style="display: block; margin: auto;" /&gt;


.footnote[Ingraham, C. (2019) ["You’ve been reading charts wrong. Here’s how a pro does it."](https://www.washingtonpost.com/business/2019/10/14/youve-been-reading-charts-wrong-heres-how-pro-does-it/), The Washington Post, 14 Oct.]


---

.question[
Is this visualization telling the complete story? What's missing?
]

&lt;img src="img/09/diminishing-return.jpg" width="50%" style="display: block; margin: auto;" /&gt;

.footnote[Credit: [Statistics How To](https://www.statisticshowto.com/probability-and-statistics/descriptive-statistics/misleading-graphs/)]


---

class: middle, center

## Algorithmic bias

---

class: middle, center

## The Hathaway Effect

---

&lt;img src="img/09/hathaway.png" width="50%" style="display: block; margin: auto;" /&gt;

.footnote[["Does Anne Hathaway News Drive Berkshire Hathaway's Stock?"](https://www.theatlantic.com/technology/archive/2011/03/does-anne-hathaway-news-drive-berkshire-hathaways-stock/72661/)]

---

## The Hathaway Effect

- **Oct. 3, 2008** - Rachel Getting Married opens: .vocab[BRK.A up .44%]

- **Jan. 5, 2009** - Bride Wars opens: .vocab[BRK.A up 2.61%]

- **Feb. 8, 2010** - Valentine’s Day opens: .vocab[BRK.A up 1.01%]

- **March 5, 2010** - Alice in Wonderland opens: .vocab[BRK.A up .74%]

- **Nov. 24, 2010** - Love and Other Drugs opens: .vocab[BRK.A up 1.62%]

- **Nov. 29, 2010** - Anne announced as co-host of the Oscars: .vocab[BRK.A up .25%]

.footnote[[The Hathaway Effect: How Anne Gives Warren Buffet a Rise](https://www.huffpost.com/entry/the-hathaway-effect-how-a_b_830041)]
---

## Amazon's experimental hiring algorithm

- Used AI to give job candidates scores ranging from one to five stars - much like shoppers rate products on Amazon, some of the people said
- Company realized its new system was not rating candidates for software developer jobs and other technical posts in a gender-neutral way
- Amazon’s system taught itself that male candidates were preferable

&gt;Gender bias was not the only issue. Problems with the data that underpinned the models’ judgments meant that unqualified candidates were often recommended for all manner of jobs, the people said.
.footnote[Dastin, J. (2018) [Amazon scraps secret AI recruiting tool that showed bias against women](https://reut.rs/2Od9fPr), Reuters, 10 Oct.]

---

## Bias in health care risk algorithm

- Algorithm to target patients for “high-risk care management” programs that seek to improve the care of patients with complex health needs by providing additional resources
- Such program are considered effective at improving outcomes and satisfaction while reducing costs, but are themselves costly -&gt; want to identify patients who would benefit the most 
- Algorithm’s designers used previous patients’ health care spending as a proxy for medical need
  - Assigned patients a "risk score", where higher risk meant more complex needs and therefore priority 

--

What happened: Algorithm tended to assign lower risk score to black patients

Obermeyer, Z., Powers, B., Vogeli, C., &amp; Mullainathan, S. (2019). [Dissecting racial bias in an algorithm used to manage the health of populations](https://science.sciencemag.org/content/366/6464/447/tab-figures-data), Science.
---

## Bias in health care risk algorithm

&lt;img src="img/09/illness_expenditure.png" width="45%" style="display: block; margin: auto;" /&gt;

&gt; Issue: health care spending does not equal need. 

---

### Bias in health care risk algorithm

&lt;img src="img/09/risk_expenditure.png" width="45%" style="display: block; margin: auto;" /&gt;

---

### Bias in health care risk algorithm

&lt;img src="img/09/risk_illness.png" width="45%" style="display: block; margin: auto;" /&gt;



&gt; Even though black patients tend to have more severe medical conditions, algorithm is built to predict health care costs rather than illness


---


## Bias in algorithms used for sentencing

&lt;img src="img/09/propublica-criminal-sentencing.png" width="70%" style="display: block; margin: auto;" /&gt;

There’s software used across the country to predict future criminal activity. And it's biased...

[Pro Publica, May 23, 2016](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)

---

class: middle

&gt;“Although these measures were crafted with the best of intentions, I am concerned that they inadvertently undermine our efforts to ensure individualized and equal justice,” he said, adding, “they may exacerbate unwarranted and unjust disparities that are already far too common in our criminal justice system and in our society.”
&gt; Then  U.S. Attorney General Eric Holder (2014)
---

## ProPublica analysis

.vocab[Data:]

Risk scores assigned to more than 7,000 people arrested in Broward County, Florida, in 2013 and 2014 + whether they were charged with new crimes over the next two years

---

## ProPublica analysis

.vocab[Results:]

- 20% of those predicted to commit violent crimes actually did

--

- Algorithm had higher accuracy (61%) when full range of crimes taken into account (e.g. misdemeanors)

--

- Algorithm was more likely to falsely flag African American defendants as higher risk, at almost twice the rate as Caucasian defendants

&lt;img src="img/09/propublica-results.png" width="90%" style="display: block; margin: auto;" /&gt;


---

class: middle, center 

Read more at 

[propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing).

---

class: middle, center

## Activity: Algorithmic Bias

---

## Discuss 

- Prompt: Think of a time when you have shared a photo of you and your friends to social media

  - Have you ever had to crop a photo that you shared?
  
  - How did you decide what to crop out?
  
---

## Activity

- Crop the images as if you were uploading them to social media


---


## Discuss 

- What images were challenging for you to decide how to crop?

- Which images did you and your partner crop the same (or similarly)?

- Which images did you and your partner crop differently?

--

- How might the cropping data here be biased?

- How might we address these biases?

---


## Watch

.center[
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/Ok5sKLXqynQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;

Are We Automating Racism?
by Glad You Asked
]

---

## Prompt

- Using shared definition/understanding of racism, do you think the algorithm used by Twitter was responsible for prejudice, discrimination, or antagonism?

--

- How was the Twitter cropping algorithm trained? 

- According to the video, where is a potential source of bias when training similar cropping algorithms?

---

## Twitter's response

- ["Algorithmic bug bounties"](https://www.theguardian.com/technology/2021/aug/10/twitters-image-cropping-algorithm-prefers-younger-slimmer-faces-with-lighter-skin-analysis)

- Conducted an internal study using photos of celebrities from Wikidata


--

&lt;img src="img/09/twitter_findings.png" width="70%" style="display: block; margin: auto;" /&gt;

.footnote[Source (and read more here): https://arxiv.org/abs/2105.08667]

---

class: middle, center

## Privacy

---

## OK Cupid Data Breach

- In 2016, researchers published data of 70,000 OkCupid users—including usernames, political leanings, drug usage, and intimate sexual details.

&gt;"Some may object to the ethics of gathering and releasing this data. However, all the data found in the dataset are or were already publicly available, so releasing this dataset merely presents it in a more useful form.""
&gt; Researchers Emil Kirkegaard and Julius Daugbjerg Bjerrekær
- Although the researchers did not release the real names and pictures of the OkCupid users, critics noted that their identities could easily be uncovered from the details provided—such as from the usernames.

&lt;br&gt;

&lt;small&gt;[*OKCupid Study Reveals the Perils of Big-Data Science*](https://www.wired.com/2016/05/okcupid-study-reveals-perils-big-data-science/)&lt;/small&gt;

---

class: middle

.question[
In analysis of data individuals willingly shared publicly on a given platform (e.g. social media data), how do you make sure you don't violate reasonable expectations of privacy?
]

&lt;img src="img/09/okcupid-tweet.png" width="70%" style="display: block; margin: auto;" /&gt;
---


## Facebook &amp; Cambridge Analytica

&lt;img src="img/09/facebook-cambridge-analytica-explained.jpg" width="60%" style="display: block; margin: auto;" /&gt;

&lt;br&gt;

[How Cambridge Analytica turned Facebook 'likes' into a lucrative political tool](https://www.theguardian.com/technology/2018/mar/17/facebook-cambridge-analytica-kogan-data-algorithm)

---

class: middle, center

## Simpson's paradox

---

## Simpson's paradox

- Not considering an important variable when studying a relationship can result in &lt;font class="vocab"&gt;Simpson's paradox&lt;/font&gt;, a phenomenon in which the omission of one explanatory variable can affect the measure of association between another explanatory variable and a response variable. 


- In other words, the inclusion of a third variable in the analysis can change the apparent relationship between the other two variables. 

---

## Simpson's paradox

&lt;img src="11-data-ethics_files/figure-html/simpsons_plot-1.png" style="display: block; margin: auto;" /&gt;

---

## Simpson's paradox 

&lt;img src="11-data-ethics_files/figure-html/simpsons_plot2-1.png" style="display: block; margin: auto;" /&gt;

---

## Berkeley Admissions

UC Berkeley admission figures for Fall of 1973:

| | Admit | Deny | Total |
|----|----|-----|-----|
|Men | 3738 |  4704 | 8442|
|Women| 1494 | 2827 | 4321 |
|Total| 5232 | 7531 | 12763|

&lt;br&gt;

.question[
- What is probability of admission?
]

--

.question[
`\(P(\text{admission}) = \dfrac{\text{# admitted}}{\text{# applied}} = \dfrac{5232}{12763} \approx 0.41\)`
]

---


## Practicing with probabilities

| | Admit | Deny | Total |
|----|----|-----|-----|
|Men | 3738 |  4704 | 8442|
|Women| 1494 | 2827 | 4321 |
|Total| 5232 | 7531 | 12763|

.question[
- `\(\small{P(\text{admit among men}) =}\)` ?
- `\(\small{P(\text{admit among women})=}\)` ? 
]

--

.question[
- `\(P(\text{admit among men}) = \dfrac{3738}{8442} \approx 0.44\)` 
- `\(P(\text{admit among women}) = \dfrac{1494}{4321} \approx 0.35\)` 
]

---

## Berkeley Admissions

- Study of gender bias

  - Men who applied were more likely than women to be admitted (44% vs 35%)
  
  - The difference was so large that it was unlikely to be due to chance!
  
--

- However, the data given omitted information about the departments that the people were applying to

  - We will look at the six largest departments: A, B, C, D, E, F

---

## Glimpse of data in tidy form




```
## Rows: 4,526
## Columns: 3
## $ department &lt;fct&gt; A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A,…
## $ gender     &lt;chr&gt; "male", "male", "male", "male", "male", "male", "male", "ma…
## $ decision   &lt;fct&gt; admit, admit, admit, admit, admit, admit, admit, admit, adm…
```

.footnote[
[https://en.wikipedia.org/wiki/Simpson%27s_paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox)
]

---

## Overall distribution of acceptance by gender

.question[
  What type of visualization would be appropriate for representing this data?
]


```
## # A tibble: 4 × 4
## # Groups:   gender [2]
##   gender decision     n prop_admit
##   &lt;chr&gt;  &lt;fct&gt;    &lt;int&gt;      &lt;dbl&gt;
## 1 female deny      1278      0.696
## 2 female admit      557      0.304
## 3 male   deny      1493      0.555
## 4 male   admit     1198      0.445
```

---

## Overall distribution of acceptance

&lt;img src="11-data-ethics_files/figure-html/berkley_hist-1.png" style="display: block; margin: auto;" /&gt;

---


## Closer look

Let's look at data from the six largest departments, labeled A-F:

| Department | Female: Admit | Female: Total | Male: Admit | Male: Total
|------------|---------|-----|-------|------|
| A       | 89      | 108  | 511   | 825|
| B   | 17     | 25  | 353   | 560 |
| C   | 202      | 593  | 120    | 227 |
| D |  131     | 375  | 138    | 417 |
| E   | 94       | 393   | 54    | 191|
| F      | 24     | 341 | 22   | 373




---

### UC Berkeley admissions: within a given department

.question[
  Within each department, what is the probability of admittance for each gender? 
]

| Department | Female: Admit | Female: Total | Male: Admit | Male: Total|
|------------|---------|-----|-------|------|
| A       | 89      | 108  | 511   | 825|
| B   | 17     | 25  | 353   | 560 |
| C   | 202      | 593  | 120    | 227 |
| D |  131     | 375  | 138    | 417 |
| E   | 94       | 393   | 54    | 191|
| F      | 24     | 341 | 22   | 373|

---



## Distribution of acceptance by department



```r
admissions %&gt;%
  count(department, gender, decision) %&gt;%
  group_by(department, gender) %&gt;%
  mutate(prop_admit = n / sum(n)) 
```

```
## # A tibble: 24 × 5
## # Groups:   department, gender [12]
##    department gender decision     n prop_admit
##    &lt;fct&gt;      &lt;chr&gt;  &lt;fct&gt;    &lt;int&gt;      &lt;dbl&gt;
##  1 A          female deny        19      0.176
##  2 A          female admit       89      0.824
##  3 A          male   deny       314      0.381
##  4 A          male   admit      511      0.619
##  5 B          female deny         8      0.32 
##  6 B          female admit       17      0.68 
##  7 B          male   deny       207      0.370
##  8 B          male   admit      353      0.630
##  9 C          female deny       391      0.659
## 10 C          female admit      202      0.341
## # … with 14 more rows
```

---



## Distribution of acceptance by department

&lt;img src="11-data-ethics_files/figure-html/berkley_hist_facet-1.png" style="display: block; margin: auto;" /&gt;

---

## UC Berkeley admissions: closer look

| Department | Female: Total | Female: Acceptance | Male: Total | Male: Acceptance |
|------------|---------|-----|-------|------|
| A       | 108      |  0.82 | 825 | 0.62 |
| B   | 25     |  0.68  | 560   | 0.63 |
| C   | 593     | 0.34  | 227    | 0.37 |
| D |  375    | 0.35  | 417    | 0.33 |
| E   | 393      | 0.24   | 191    | 0.28 |
| F      | 341     | 0.07 | 373   | 0.06 |

--


- Rank departments by total number of male applicants


- Rank departments by total number of female applicants

--

.question[Are the departments uniform in their admission rates?] 

--

  - Notice how **A** and **B** have highest acceptance! What does this mean?

---

## UC Berkeley admissions: plot

&lt;img src="img/08/bickel.png" width="400" style="display: block; margin: auto;" /&gt;

&lt;br&gt;

.footnote[
  [https://homepage.stat.uiowa.edu/~mbognar/1030/Bickel-Berkeley.pdf](https://homepage.stat.uiowa.edu/~mbognar/1030/Bickel-Berkeley.pdf)
]


---

.question[
...smoking cigarettes can help you live longer?
]

&lt;img src="img/09/cigarettes1.png" width="50%" style="display: block; margin: auto;" /&gt;

.footnote[ [How Charts Lie: Getting Smarter About Visual Information](https://wwnorton.com/books/9781324001560) ]

---

&gt; Missing information


&lt;img src="img/09/cigarettes2.png" width="50%" style="display: block; margin: auto;" /&gt;

.footnote[ [How Charts Lie: Getting Smarter About Visual Information](https://wwnorton.com/books/9781324001560) ]

---


&gt; Simpson's paradox


&lt;img src="img/09/cigarettes3.png" width="50%" style="display: block; margin: auto;" /&gt;

.footnote[ [How Charts Lie: Getting Smarter About Visual Information](https://wwnorton.com/books/9781324001560) ]

---

&gt; Individual level


&lt;img src="img/09/cigarettes4.png" width="70%" style="display: block; margin: auto;" /&gt;

.footnote[ [How Charts Lie: Getting Smarter About Visual Information](https://wwnorton.com/books/9781324001560) ]

---

class: center, middle

## Further study on data science ethics

---

## Further reading


.pull-left[
&lt;img src="img/09/ethics-data-science.jpg" width="60%" style="display: block; margin: auto;" /&gt;
]

.pull-right[
[Ethics and Data Science](https://www.amazon.com/Ethics-Data-Science-Mike-Loukides-ebook/dp/B07GTC8ZN7)  

by Mike Loukides, Hilary Mason, DJ Patil  

(free Kindle download)
]

---

## Further reading

.pull-left[
&lt;img src="img/09/how-charts-lie.jpg" width="60%" style="display: block; margin: auto;" /&gt;
]

.pull-right[
[How Charts Lie: Getting Smarter About Visual Information](https://wwnorton.com/books/9781324001560)  

by Alberto Cairo
]



---

## Further reading

.pull-left[
&lt;img src="img/09/weapons-of-math-destruction.jpg" width="60%" style="display: block; margin: auto;" /&gt;

]
.pull-right[
[Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy ](https://www.amazon.com/Ethics-Data-Science-Mike-Loukides-ebook/dp/B07GTC8ZN7)  

by Cathy O'Neil
]

---

## Further watching

.center[
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/MfThopD7L1Y" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;

Predictive Policing: Bias In, Bias Out  
by Kristian Lum 
]

---

## Parting thoughts

- At some point during your data science journey you will learn tools that can be used unethically

- You might also be tempted to use your knowledge in a way that is ethically questionable either because of business goals or for the pursuit of further knowledge (or because your boss told you to do so)

.question[
How do you train yourself to make the right decisions (or reduce the likelihood of accidentally making the wrong decisions) at those points?
]

---

## Do good with data

- Data Science for Social Good:

  - [at the University of Chicago](http://www.dssgfellowship.org/)
  - [at the Alan Turing Institute](https://www.turing.ac.uk/collaborate-turing/data-science-social-good)

- [DataKind](https://www.datakind.org/): DataKind brings high-impact organizations together with leading data scientists to use data science in the service of humanity 
- [Pledge to promote data values &amp; practices](https://datapractices.org/manifesto/)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"slideNumberFormat": "%current%",
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
