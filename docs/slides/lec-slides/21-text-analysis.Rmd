---
title: "Text analysis"
author: "Becky Tang"
date: "07.08.2021"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: "sta199-slides.css"
    logo: img/sta199-sticker-icon.png
    lib_dir: libs/font-awesome
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: "%current%" 
      ratio: "16:9"
---


layout: true

<div class="my-footer">
<span>
<a href="http://datasciencebox.org" target="_blank">datasciencebox.org</a>
</span>
</div> 

```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warm = 1
  )
# Set dpi and height for images
knitr::opts_chunk$set(fig.height = 3, fig.width = 5, dpi = 300, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.align = "center") 
# ggplot2 color palette with gray
color_palette <- list(gray = "#999999", 
                      salmon = "#E69F00", 
                      lightblue = "#56B4E9", 
                      green = "#009E73", 
                      yellow = "#F0E442", 
                      darkblue = "#0072B2", 
                      red = "#D55E00", 
                      purple = "#CC79A7")
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
# For magick
dev.off <- function(){
  invisible(grDevices::dev.off())
}
# For ggplot2
ggplot2::theme_set(ggplot2::theme_bw())
```

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(broom)
library(knitr)
library(DT)
library(openintro)
library(infer)
library(patchwork)
library(kableExtra)
library(lubridate)
```

---


## Packages

In addition to `tidyverse` we will be using a few other packages today.

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidytext)
library(wordcloud)
```

---



## Tidy Data

.question[
What makes a data frame tidy?
]

--

1. Each variable must have its own column.
2. Each observation must have its own row.
3. Each value must have its own cell.

---


## Tidytext

- Using tidy data principles can make many text mining tasks easier, more 
effective, and consistent with tools already in wide use.

- Learn more at https://www.tidytextmining.com/.

---

## What is tidy text?

- Thus far in class you have been employing tidy data principles, and we will continue to do so here!

- Specifically, we will work in the 'tidy text format': a table with one token per row

```{r introduce_text}
text <- c("Thus far in class you have been employing tidy data principles, and we will continue to do so here!", 
          "Specifically, we will work in the 'tidy text format': a table with one token per row")
text
```

- How can we make this text into a format we can work with?

---

## What is tidy text?

```{r text-df, message = F, warning = F}
text_df <- tibble(line = 1:2, text = text)
text_df
```

---

## Tokens

- A token is a meaningful unit of text. For us, that will be a single word
- Tokenizing is simply splitting a body of text into tokens, which can achieved using the function `unnest_tokens()`

```{r unnest-token, echo = T}
tidy_text <- text_df %>%
  unnest_tokens(word, text)
tidy_text
```

---

## Tokenizing

- With the `unnest_tokens()` function, we can easily format any body of text into a user-friendly data frame
- First argument: name of column/variable that text is being unnested into
- Second argument: name of column/variable that is to be unnested 
- Other details:
  - All other columns kept
  - Punctuation removed
  - Defaults is to convert tokens to lowercase

```{r tokenize, echo =F}
tidy_text %>%
  slice(1:3)
```

---

## Democratic candidate tweets

- Time to work with some fun data!
- Tweets from two Democratic candidates during the 2019 campaign: Joe Biden and Elizabeth Warren

```{r tweet-data, echo = F}
tweets <- read.csv("data/dem_cand_tweets_2019_10_02.csv") %>%
  filter(screen_name %in% c("ewarren", "JoeBiden"))
tweets <- tweets %>%
  filter(!str_detect(text, "^RT")) %>%   #get rid of retweets
  mutate(timestamp = ymd_hms(created_at)) %>%
  select("timestamp","screen_name", "text")
tweets %>%
  select("screen_name", "text") %>%
  slice(1:3)
```

---

## Tidy tweets

```{r tidy-tweet, warning = F}
remove_reg <- "&amp;|&lt;|&gt;"
tidy_tweets <- tweets %>%
  mutate(text = str_remove_all(text, remove_reg))%>%
  unnest_tokens(word, text, token = "tweets")
tidy_tweets %>% 
  slice(1:5)
```

---

## Word counts

What are the most commonly tweeted words by these candidates?

```{r word_count, cache = T}
counts <- tidy_tweets %>%
  count(word, sort = T)
counts %>%
  slice(1:10)
``` 

---

## Stop words

- In computing, stop words are words which are filtered out before or after 
processing of natural language data (text).

- They usually refer to the most common words in a language, but there is not a 
single list of stop words used by all natural language processing tools.

```{r stop_words, echo = T, eval = T}
data("stop_words")
head(stop_words)
```


---
## Stop words

Let's remove the stop words and find the most common words again:

```{r remove_stop, cache = T}
tidy_tweets <- tidy_tweets %>%
  anti_join(stop_words)
tidy_tweets %>%
  count(word, sort = T) %>%
  slice(1:10)
```

---

## Word frequency

While it's nice to get raw counts, we may prefer to know which words are used more often *relative* to other words. For this we can look towards word frequencies:

```{r frequency}
frequency_all <-  tidy_tweets %>%
  count(word, sort = T) %>%
  mutate(freq = n / sum(n)) 
```

```{r freq-plot, fig.height=1.75, echo = F}
ggplot(frequency_all %>% top_n(10, freq) %>%
         mutate(word = reorder(word, freq)), aes(x = word, y = freq))+
  geom_col()+ 
  coord_flip() 
```



---

## Frequency by candidate

```{r freq_by_cand, echo = F}
frequency <- tidy_tweets %>%
  group_by(screen_name) %>%
  count(word, sort = T) %>%
  left_join(tidy_tweets %>%
              group_by(screen_name) %>%
              summarise(total = n()),
            by = "screen_name") %>%
  mutate(freq = n / total)
frequency %>% 
  slice(1:4)
```

---

class: middle, center

## Sentiment analysis

---

## Sentiment analysis

- One way to analyze the sentiment of a text is to consider the text as a 
combination of its individual words 

- The sentiment content of the whole text is the sum of the sentiment content of
the individual words

- The sentiment attached to each word is given by a *lexicon*, which may be 
downloaded from external sources

---

## Sentiment lexicons

- AFINN assigns each word an integer score between -5 and 5. Negative scores indicate negative sentiment, and positive scores indicate the opposite

- The bing lexicon categorizes words into one of two categories: 'positive' or 'negative'

.pull-left[
```{r afinn}
get_sentiments("afinn") %>%
  slice(1:6)
```
]

--

.pull-right[
```{r bing}
get_sentiments("bing") %>%
  slice(1:6)
```
]


---

## Notes about sentiment lexicons

Not every word is in a lexicon!

```{r miss_data}
get_sentiments("bing") %>% 
  filter(word == "data")
```

--

Lexicons do not account for qualifiers before a word (e.g., "not happy") because they were constructed for one-word tokens only

--

- Summing up each word's sentiment may result in a neutral sentiment, even if there are strong positive and negative sentiments in the body

---

## Sentiment of Elizabeth Warren's tweets: bing lexicon

We will use the bing lexicon to estimate the sentiment of Elizabeth Warren's tweets:

```{r warren_sent}
(warren_sent <- tidy_tweets %>%
  filter(screen_name == "ewarren") %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(sentiment))

warren_sent%>%
  pull(n) %>%
  diff()
```

It appears that Warren is pretty neutral...or is she?

---

## Sentiment of Elizabeth Warren's tweets by month

What if we look at Warren's tweets by month?

```{r warren_sent_month, echo = F}
tidy_tweets %>%
  filter(screen_name == "ewarren") %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(months(timestamp), sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  rename(month = 1)
```

---

## Most common positive/negative words

```{r pos_neg, echo = T}
bing_counts <- tidy_tweets %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(word, sentiment, sort = T)

bing_counts %>%
  slice(1:4)
```

---

## Most common positive/negative words

```{r pos_neg_plot, echo = F, fig.height=2}
bing_counts %>%
  group_by(sentiment) %>%
  top_n(10, n) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x= word, y = n, fill = sentiment))+
  geom_col(show.legend = F) +
  facet_wrap(~sentiment, scales = "free_y")+
  labs(y = "Contribution to sentiment", x = NULL) + 
  coord_flip()
```
---

## Customize stop words

- For this analysis, we should consider removing the word 'trump' as a word with positive connotation
- To do so, we can make a custom list of stop-words:

```{r custom_stop, echo = T}
my_stop_words <- tibble(word = c("trump"), lexicon = c("custom"))
custom_stop_words <- bind_rows(my_stop_words, stop_words)
custom_stop_words 
```

---

##  Most common positive/negative words

```{r pos_neg_custom, echo = F, fig.height=2}
tidy_tweets %>%
  anti_join(custom_stop_words) %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(word, sentiment, sort = T) %>%
  group_by(sentiment) %>%
  top_n(10, n) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x= word, y = n, fill = sentiment))+
  geom_col(show.legend = F) +
  facet_wrap(~sentiment, scales = "free_y")+
  labs(y = "Contribution to sentiment", x = NULL) + 
  coord_flip()
```


---

## Wordcloud

We can visualize the frequencies using a word cloud:

```{r wordcloud_code, cache = T, eval = F}
tidy_tweets %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 40,scale=c(3,.5)))
```

---

## Wordcloud

```{r wordcloud, cache = T, echo = F,  fig.height = 2}
tidy_tweets %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 40,scale=c(1,.5)))
```

---

## Sentiment Wordcloud

```{r sent_cloud, echo = F, cache = T, fig.height = 2}
custom_tidy_tweets <- tidy_tweets %>%
  filter(!word %in%custom_stop_words$word,
         !word %in% str_remove_all(custom_stop_words$word, "'"),
         str_detect(word, "[a-z]")) 
custom_tidy_tweets%>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(word, sentiment, sort = T) %>%
  reshape2::acast(word ~ sentiment, value.var = "n", fill = 0) %>%   # turn data frame into matrix
  comparison.cloud(max.words = 40, scale=c(1,.5),
                   title.size = 1)
```

---

## Word frequencies of Biden vs. Warren

We can visualize the differences and similarities in word frequencies between two candidates: 

```{r word_freq, echo = F, warning = F, fig.height=1.5}
frequency <- frequency %>%
  select(screen_name, word, freq) %>%
  spread(screen_name, freq) %>%
  arrange(ewarren, JoeBiden)

ggplot(frequency, aes(JoeBiden, ewarren))+
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +
  geom_text(aes(label = word), check_overlap = T, vjust = 1.5)+
  scale_x_log10(labels = scales::percent_format())+
  scale_y_log10(labels = scales::percent_format())+
  geom_abline(color = "blue")
```

- Words near the blue line are used with roughly equal frequencies between Joe Biden and Elizabeth Warren
- Words far away from the blue line are favored much more by one candidate than the other

---

## Probability of using a word

- We can also explore questions such as: given a word, which candidate is more likely to use that word in a tweet?



- We will utilize the log-odds ratio. Odds ratio for Candidate A versus Candidate B
$$\text{OR}_{A:B}(word) = \dfrac{\text{odds}_A}{\text{odds}_B} = \dfrac{\text{Prob(A uses word)}}{\text{Prob(B uses word)}}$$


- The probability that Candidate A uses the word is the number of times A used the word, divided by the total number of words used by A: 
$$\text{Prob(A uses word)} = \dfrac{n_A}{total_A}$$

---

## Log-odds ratio

$$ \log \text{OR}_{A:B}(word)  = \log \left( \dfrac{n_A / total_A}{n_B / total_B}\right) $$

- Equal odds corresponds to OR = 1, which corresponds to log(OR) = 0

- If candidate A uses the word with higher probability, then log(OR) > 0

- We use the following approximation in case a candidate does not use the word at all:

$$ \log \text{OR}_{A:B}(word) \approx \log \left( \dfrac{(n+1)_A / (total+1)_A}{(n+1)_B / (total+1)_B}\right)$$
---

## Word usage: equally likely

Log-odds ratios for Biden versus Warren displayed in ascending order of absolute value of log-odds ratio

```{r word-usage-likely, echo = F }
candidates <- c("JoeBiden", "ewarren")
word_ratios <- tidy_tweets %>%
  filter(screen_name %in% candidates) %>%
  filter(!str_detect(word, "^@")) %>%
  count(word, screen_name) %>%
  group_by(word) %>%
  filter(sum(n) >= 10) %>% # only consider more frequently uses words
  ungroup() %>%
  spread(screen_name, n, fill = 0) %>%
  mutate_if(is.numeric, list(~(. +1) / (sum(.) + 1))) %>%
  mutate(logratio = log(eval(parse(text = candidates[1])) / eval(parse(text = candidates[2])))) %>%
  select(word, logratio)%>%
  arrange(desc(logratio))
word_ratios %>%
  arrange(abs(logratio)) 
```

- Words about equally likely to be tweeted from the two candidates (log(OR) $\approx$ 0) are non-buzzwords

---

## Word usage: most distinctive 

- The words that are most likely to be from Biden and not Warren will have the largest, most positive ratios
- The words that are most likely to be from Warren and not Biden will have the smallest, most negative ratios
- The plot on the next slides shows the 16 most positive and negative ratios

---

## Word usage: most distinctive cont.

```{r word-usage-diff, echo = F, fig.height = 2.25}
word_ratios %>% 
  group_by(logratio < 0) %>%
  top_n(16, abs(logratio)) %>%
  ungroup() %>%
  mutate(word = reorder(word, logratio))%>%
  ggplot(aes(word, logratio, fill = logratio < 0)) +
  geom_col(show.legend = F) +
  coord_flip() +
  ylab(paste("log odds ratio", candidates[1], "/", candidates[2])) +
  scale_fill_discrete(name = "", labels = c(candidates[1], candidates[2]))+
  theme(axis.text.y = element_text(size = 5))

```


---

## Additional resources

[Text Mining with R](https://www.tidytextmining.com/)
- Chapter 1: The tidy text format
- Chapter 2: Sentiment analysis with tidy data

--- 

## Announcements

- Exam 02 released tomorrow, July 9. Due Sunday, July 11 at 11:59pm. No late work will be accepted.

- Project proposal is due tomorrow, July 9 at 11:59pm. Upload a PDF to Gradescope as well.
