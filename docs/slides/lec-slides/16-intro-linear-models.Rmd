---
title: "Introducing linear models"
author: "Becky Tang"
date: "11/11/2022"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: "math118-slides.css"
    lib_dir: libs/font-awesome
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: "%current%" 
      ratio: "16:9"
editor_options: 
  chunk_output_type: console
---


layout: true

<div class="my-footer">
<span>
<a href="http://datasciencebox.org" target="_blank">datasciencebox.org</a>
</span>
</div> 


```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warm = 1
  )
# Set dpi and height for images
knitr::opts_chunk$set(fig.height = 3, fig.width = 5, dpi = 300, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.align = "center") 
# ggplot2 color palette with gray
color_palette <- list(gray = "#999999", 
                      salmon = "#E69F00", 
                      lightblue = "#56B4E9", 
                      green = "#009E73", 
                      yellow = "#F0E442", 
                      darkblue = "#0072B2", 
                      red = "#D55E00", 
                      purple = "#CC79A7")
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
# For magick
dev.off <- function(){
  invisible(grDevices::dev.off())
}
# For ggplot2
ggplot2::theme_set(ggplot2::theme_bw())
```

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(broom)
library(knitr)
library(DT)
library(openintro)
library(infer)
library(patchwork)
library(kableExtra)
```

---


class: middle, center

## The language of models

---

## Modeling

- We use models to...
  - understand relationships
  - assess differences
  - make predictions


- We will focus on .vocab[linear] models but there are many other types.

---

class: center, middle

# Data: Teacher salaries

---

## Teacher salaries

This data set contains teacher salaries from 2009-2010 for 71 teachers employed by the St. Louis Public School in Michigan.

```{r message=FALSE, echo= F}
data("teacher")
set.seed(1)
grade <- sample(c(rep("elementary", 20), rep("middle", 25), rep("high", 26)))
teacher <- teacher %>%
  mutate(degree = factor(degree, c("MA","BA")),
         grade = grade)
```

```{r}
glimpse(teacher)
```

---

## Data

- `degree`: highest educational degree attained (`"BA"` or `"MA"`)

- `fte`: full-time enrollment status (fulled time 1 or part-time 0.5)

- `years`: number of years employed by the school district

- `base`: base annual salary, in dollars

- `fica`: amount paid into Social Security and Medicare per year, in dollars

- `retirement`: amount paid into the retirement fund of the teacher per year, in dollars

- `total`: total annual salary of the teacher, resulting from the sum of `base` + `fica` + `retirement`, in dollars.


---

class: center, middle

## Modeling the relationship between variables

---

## Describe the distribution of base

.midi[
```{r, fig.height = 2}
ggplot(data = teacher, aes(x = base)) +
  geom_histogram(bins = 20) +
  labs(title="Distribution of base salary (in $)")
```
]

---

## Years and Degree

.pull-left[
```{r, fig.height = 4}
ggplot(data = teacher, aes(x = years)) +
  geom_histogram(bins = 20) +
  theme(text = element_text(size = 20))
```
]

.pull-right[
```{r, fig.height = 4}
ggplot(data = teacher, aes(x = degree)) +
  geom_bar() +
  theme(text = element_text(size = 20))
```
]


---

## Models as functions

- We can represent relationships between variables using **functions**

- A .vocab[function] in the *mathematical* sense is the relationship between one or more inputs and an output created from those inputs. 
    - Plug in the inputs and receive back the output
    
--

- The formula $y = 3x + 7$ is a function with input $x$ and output $y$.
  - When $x$ is $5$, the output $y$ is $22$
    
    ```
    y = 3 * 5 + 7 = 22
    ```
    
--

  - When $x$ is $12$, the output of $y$ is $43$
  
    ```
    y = 3 * 12 + 7 = 43
    ```

---

## Visualizing the linear model

.midi[
```{r warning = FALSE, fig.height = 2}
ggplot(data = teacher, aes(x = years, y = base)) +
  geom_point() +
  ggtitle("Base salary vs years") +
  geom_smooth(method = "lm")  #<<
```
]

---


## Visualizing the linear model

.midi[
```{r warning = FALSE, fig.height = 2}
ggplot(data = teacher, aes(x = years, y = base)) +
  geom_point() +
  ggtitle("Base salary vs years") +
  geom_smooth(method = "lm", se = FALSE, #<<
              col = "red", lty = 2, lwd = 1) #<<
```
]

---

## Vocabulary

- .vocab[Response variable]: Variable whose behavior or variation you are trying to understand, on the y-axis. Also called the **dependent variable**, and referred to as $y$. For linear 

- .vocab[Explanatory variables]: Other variables that you want to use to explain the variation in the response, on the x-axis. Also called **independent variables**, **predictors**, or **features**. Each explanatory variable is referred to using an $x$.

--

.question[What might be the dependent and explanatory variables in our `teacher` data?]

--

- .vocab[Predicted value]:</font> Output of the **model function**
    - The model function gives the typical value of the response variable
    *conditioning* on the explanatory variables (what does this mean?)

---

## Data

.pull-left[
```{r}
teacher %>%
  dplyr::select(base, years) %>%
  slice(1:5)

```
]

.pull-right[
```{r}
teacher %>%
  dplyr::select(base, years) %>%
  rename("y" = 1, "x" = 2) %>%
  slice(1:5)
```
]


---

## The linear model with a single predictor

- .vocab[Linear regression] is the statistical method for fitting a line to data where the relationship between predictor $x$ and quantitative response $y$ can be modeled by a straight line with some error $\epsilon$:

$$y  = \beta_{0} + \beta_{1}x + \epsilon$$
  - For this class, we will not worry about the $\epsilon$ too much. But we should remember there is always a little bit of error!
  
  - When we ignore $\epsilon$, we replace $y$ with $\hat{y}$
  
---

## The linear model with a single predictor (cont.)

- Thus, we have the following model:

$$\hat{y} = \beta_0 + \beta_1~x$$

where $\beta_1$ is often called the .vocab[coefficient] or  .vocab[slope] for the explanatory variable $x$

--

- Unfortunately, we can't get these values

--

- So we use sample statistics to estimate them:

$$\hat{y} = b_0 + b_1~x$$

---



## Vocabulary

- .vocab[Residuals]: Shows how far each case $i$  is from its predicted value
   - **Residual = Observed value - Predicted value = $y_{i} - \hat{y}_{i}$**
   - Tells how far above/below the model function each case is

---

.question[
What does a positive residual mean? Which salaries on the plot have positive residuals, those below or above the line?
]

```{r warning = FALSE, echo=FALSE, fig.height = 2}
ggplot(data = teacher, aes(x = years, y = base)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

---

## Residuals

```{r warning = FALSE, fig.height=2.25, fig.width=5 ,echo = FALSE}
mod <- lm(base ~ years, data  = teacher)
x <- 13
y_true <- teacher$base[teacher$years==x]
y_pred <- predict(mod, newdata = data.frame(years = x))
ggplot(data = teacher, aes(x = years, y = base)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, 
              col = "blue", lty = 1, lwd = 1) + 
  geom_point(aes(x = x, y = y_true), col = "orange", shape = 1, size = 4)
```

---

## Residuals

- True salary when years = 13: 61216

- Predicted salary when years = 13: 54108.45 

```{r warning = FALSE, fig.height=2, fig.width=5 ,echo = FALSE}
lab <- paste0(y_true, " - ", round(y_pred,2), " = ", y_true - round(y_pred,2))
ggplot(data = teacher, aes(x = years, y = base)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, 
              col = "blue", lty = 1, lwd = 1) + 
  geom_point(aes(x = x, y = y_true), col = "orange", shape = 1, size = 4) + 
  geom_segment(aes(x = x, y = y_true, xend = x, yend = y_pred), col = "orange") + 
    geom_point(aes(x = x, y = y_pred), col = "blue", shape = 1, size = 2) + 
  annotate("text", x  = x + 7, y = y_pred - 5000, label =lab, color = "orange", size = 4)
```

---

## Residuals
   
```{r warning = FALSE, fig.height=2.25, fig.width=5 ,echo = FALSE}
x <- 34
y_true <- teacher$base[teacher$years==x]
y_pred <- predict(mod, newdata = data.frame(years = x))
ggplot(data = teacher, aes(x = years, y = base)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, 
              col = "blue", lty = 1, lwd = 1) + 
  geom_point(aes(x = x, y = y_true), col = "orange", shape = 1, size = 4)
```

---

## Residuals

- True salary when years = 34: 65360

- Predicted salary when years = 34: 70973.64 

```{r warning = FALSE, fig.height=2, fig.width=5 ,echo = FALSE}
ggplot(data = teacher, aes(x = years, y = base)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, 
              col = "blue", lty = 1, lwd = 1) + 
  geom_point(aes(x = x, y = y_true), col = "orange", shape = 1, size = 4) + 
  geom_segment(aes(x = x, y = y_true, xend = x, yend = y_pred), col = "orange") + 
    geom_point(aes(x = x, y = y_pred), col = "blue", shape = 1, size = 2) + 
  annotate("text", x  = x, y = y_true - 7000, label = "65360 -  70973.64  = -5613.64", color = "orange", size = 4)
```

---

## Models - upsides and downsides

- Models can sometimes reveal patterns that are not evident in a graph of the
data. This is a great advantage of modeling over simple visual inspection of
data. 

- There is a real risk, however, that a model is imposing structure that is
not really there on the scatter of data, just as people imagine animal shapes in
the stars. A skeptical approach is always warranted.

---

## Variation around the model...

is just as important as the model, if not more!

.question[
*Statistics is the explanation of variation in the context of what remains
unexplained.*
]

- The scatter suggests that there might be other factors that account for large parts 
of salary-to-salary variability, or perhaps just that randomness plays a big role.

- Adding more explanatory variables to a model can sometimes usefully reduce
the size of the scatter around the model. (We'll talk more about this later.)

---

## How do we use models?

1. .vocab[Explanation:] Characterize the relationship between $y$ and $x$ via *slopes* for
numerical explanatory variables or *differences* for categorical explanatory
variables

2. .vocab[Prediction:] Plug in $x$, get the predicted $y$

---

class: middle, center

## Interpreting Models

---


## Packages

.pull-left[
![](img/17/tidyverse.png)

![](img/17/broom.png)
]
.pull-right[
- You're familiar with the tidyverse:
```{r message=FALSE}
library(tidyverse)
```

- The broom package takes the messy output of built-in functions in R, such as `lm`, and turns them into tidy data frames.
```{r message=FALSE}
library(broom)
```
]

---

## broom 

.pull-left[
.middle[
![](img/17/broom.png)
]
]

.pull-right[
- **broom** follows tidyverse principles and tidies up regression output
- **`tidy`**: Constructs a tidy data frame summarizing model's statistical findings
- **`glance`**: Constructs a concise one-row summary of the model
- **`augment`**: Adds columns (e.g. predictions, residuals) to the original data that was modeled
]

[https://broom.tidyverse.org/](https://broom.tidyverse.org/)

---


## Interpeting the slope

- Recall the linear regression model:

$$\hat{\color{purple}{y}} = \beta_{0} + \beta_{1}x$$
--

- Let's increase $x$ by 1, i.e. plug in $(x+1)$ into the equation:

$$
\begin{align*}
\beta_{0} + \beta_{1}(x+1) &= \beta_{0} + \beta_{1}x + \beta_{1}\\
&= (\beta_{0} + \beta_{1}x) + \beta_{1}\\
&= \hat{\color{purple}{y}}  + \beta_{1}
\end{align*}
$$
--

- .question[What is the interpretation of the slope?]


---

## Interpeting the intercept

- Recall the linear regression model:


$$\hat{\color{purple}{y}}  = \beta_{0} + \beta_{1}x$$

--

- Let's set $x=0$:

$$\hat{\color{purple}{y}}  = \beta_{0} + \beta_{1}\times 0 = \beta_{0}$$


--

- .question[What is the interpretation of the intercept?]


---


## Years and base salary

.midi[
```{r}
mod_years <- lm(base ~ years, data = teacher)
tidy(mod_years)
```
]

--

$$\widehat{baseSalary} = \color{blue}{43668} + \color{orange}{803}\times years$$

--

- .vocab[Slope]: For each additional year a teacher is employed at the school district, the base salary is expected
to be *higher*, on average, by $\color{orange}{803}$ dollars.

--

- .vocab[Intercept]: Teachers that have worked 0 years in the district are expected to have a base salary of $\color{blue}{43668}$ dollars,
on average.
    - Does this make sense?
   
--

- Why is there a "hat" on the response variable?

---


## Least squares regression

Why these coefficients? i.e. Why this particular line? The regression line minimizes the sum of squared residuals.

--

- .vocab[Residuals]: </font> $e_i = y_i - \hat{y}_i$,


- The regression line minimizes $\sum_{i = 1}^n e_i^2$.

- Equivalently, minimizing $\sum_{i = 1}^n [y_i - (b_0 + b_1~x_i)]^2$

.question[
Why do we minimize the *squares* of the residuals?
]

---

## Visualizing residuals

```{r echo=FALSE, fig.height = 2.5}
d <- tibble(
    base     = mod_years$model$base,
    years    = mod_years$model$years,
    pred         = mod_years$fitted.values,
    res          = mod_years$residuals
  )
p <- ggplot(data = d, mapping = aes(x = years, y = base)) +
  geom_point(alpha = 0.2) + 
  theme_bw() +
  labs(title = "Years vs. salary of teachers", subtitle = "Just the data",
       y = "base salary") 
p
```

---

## Visualizing residuals (cont.)

```{r echo=FALSE, fig.height= 2.5}
p <- p + 
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(mapping = aes(y = pred)) +
  labs(subtitle = "Data + least squares regression line")
p
```

---

## Visualizing residuals (cont.)

```{r echo = FALSE, fig.height= 2.5}
p + 
  geom_segment(mapping = aes(xend = years, yend = pred), alpha = 0.4, color="orange") +
  labs(subtitle = "Data + least squares regression line + residuals")
```

Check out the applet [here](http://www.rossmanchance.com/applets/RegShuffle.htm) to see this process in action.

---

class: middle, center

## Live code: Exercises 1-3

---

class: middle, center

## Correlation does not imply causation!!

Remember this when interpreting model coefficients

---

class: center, middle

# Prediction with models

---

## Predict base salary from years

.question[
On average, how much does a teacher in this district make if they have worked for 15 years?]

--

- Recall our fitted model:

$$\widehat{baseSalary} = 43668.1 + 803.1\times years$$

--

- Let's just plug in 15 for $years$:

```{r}
43668.1  + 803.1 * 15
```

  - "On average, we expect that a teacher who has been employed for 15 years to have a base salary of $55714.60."

  - **Warning:** We "expect" this to happen, but there will be some variability.

---

## Prediction vs. extrapolation

.question[
On average, what is the base salary for someone who has worked 80 years?
]

```{r extrapolate, warning = FALSE, echo=FALSE, fig.height = 2}
newdata <- tibble(years = 80)
newdata <- newdata %>%
  mutate(base = predict(mod_years, newdata = newdata))
ggplot(data = teacher, aes(x = years, y = base)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", fullrange = TRUE, se = FALSE) +
  # xlim(0, 420) +
  # ylim(0, 320) +
  geom_segment(data = newdata, mapping = aes(x = years, y = 0, xend = years, yend = base), color = "red", lty = 2) +
  geom_segment(data = newdata, mapping = aes(x =years, y = base, xend = 0, yend = base), color = "red", lty = 2)
```

---

## Watch out for extrapolation!

> "When those blizzards hit the East Coast this winter, it proved to my satisfaction 
that global warming was a fraud. That snow was freezing cold. But in an alarming 
trend, temperatures this spring have risen. Consider this: On February 6th it was 10 
degrees. Today it hit almost 80. At this rate, by August it will be 220 degrees. So 
clearly folks the climate debate rages on."<sup>1</sup>  <br>
Stephen Colbert, April 6th, 2010
.footnote[[1] OpenIntro Statistics. "Extrapolation is treacherous." OpenIntro Statistics.]

---

class: middle, center

## Live code: Exercises 4-5

---

class: middle, center

## Categorical Predictors

---

## What about non-continuous predictors?

```{r echo=FALSE}
teacher %>% 
  select(degree, grade) %>% 
  slice(1:8)
```

---

### Categorical predictor with 2 levels

`base` salary regressed on `degree`

```{r}
mod_degree <- lm(base ~ degree, data = teacher)
tidy(mod_degree)
```

--

<br>

$$\widehat{baseSalary} = 56610 - 352\times degreeBA$$
.question[What is happening here?]

---

### Indicator variables

- Under the hood, `R` has actually converted the categorical variable `degree` into an .vocab[indicator] variable called $degreeBA$


$$degreeBA = \begin{cases}
1 & \text{ if teacher has a BA} \\
0 &  \text{ if teacher does not have a BA (i.e. has MA)}
\end{cases}$$

--

- More generally, indicator variables are .vocab[binary] variables that take the value 1 when a condition is true, and 0 otherwise

---

### Categorical predictor with 2 levels (cont.)

- The .vocab[baseline] level or category is the level of the variable when all indicators are 0

--

$$\widehat{baseSalary} = 56610 - 352\times degreeBA$$


- .question[What is the baseline level in our model?]

--

- Whenever there is a categorical variable in the model, we *always* interpret the coefficients with respect to the baseline!

---

### Categorical predictor with 2 levels (cont.)

$$\widehat{baseSalary} = \color{blue}{56610} \color{orange}{- 352}\times degreeBA$$

- Intercept $\beta_{0}$: Teachers with an MA are expected, on 
average, to have a base salary of $\color{blue}{56610}$ dollars

  - Plug in 0 for `degreeBA` and interpret accordingly
  
--

- $\beta_{1}$: Teachers with a BA are expected, on average,
to have a base salary $\color{orange}{352}$ dollars *less* than teachers with an MA

  - Notice that we explicitly compare to baseline!





---

## Regression lines by `degree`

$$\widehat{baseSalary} =  \color{blue}{56610} \color{orange}{- 352}\times degreeBA$$


```{r warning = FALSE, echo = F, fig.height=2.2}
ggplot(data = teacher%>%
  filter(base >= 50000 & base <= 60000), aes(x = years, y = base))+
    geom_point(alpha = 0.4) +
    geom_hline(yintercept = (mod_degree %>%tidy() %>% pull(estimate))[1],
               col = "blue") +
  geom_hline(yintercept = mod_degree %>%tidy() %>% pull(estimate) %>% sum(),
               col = "orange")+
   scale_y_continuous(limits=c(50000,60000)) +
  ggtitle("Regression lines by degree type",
          subtitle = "y-axis zoomed in for clarity")
  
```

---




### Categorical predictors with more than 2 levels

.midi[
```{r}
mod_grade <- lm(base ~ grade, data = teacher)
tidy(mod_grade)
```
]

.question[
What do these rows correspond to? Why are there only two `grade`s listed, but we know there are three total?)
]

---

### Categorical predictors with more than 2 levels

.midi[
```{r echo = F}
mod_grade <- lm(base ~ grade, data = teacher)
tidy(mod_grade)
```
]

- When the categorical explanatory variable has many levels, the levels are encoded 
to .vocab[dummy variables]

- Each coefficient describes the expected difference between `base` salaries in that particular `grade` level compared to the baseline level.


---

## How dummy variables are made

```{r echo=FALSE}
teacher %>% 
  select(grade) %>% 
  group_by(grade) %>% 
  sample_n(1) %>%
  mutate(
    elementary = as.integer(ifelse(grade == "elementary", 1L, 0)),
    middle    = as.integer(ifelse(grade == "middle", 1L, 0)),
    high    = as.integer(ifelse(grade == "high", 1L, 0))
  )
```

---

class: middle, center

## Live code: Exercises 6-9

---

## Tidy vs. not-so-tidy regression output

Let's revisit this model:

```{r}
mod_years <- lm(base ~ years, data = teacher)
```

---

## Not-so-tidy regression output

- You might come across these in your googling adventures, but we'll try to stay away from them

- Not because they are wrong, but because they don't result in tidy data frames as results.

---

## Not-so-tidy regression output (1)

**Option 1:**

```{r}
mod_years
```

---

##  Not-so-tidy regression output (2)

**Option 2:**

```{r}
summary(mod_years)
```

---

## Review

.question[
What makes a data frame tidy?
]

--

1. Each variable must have its own column.
2. Each observation must have its own row.
3. Each value must have its own cell.

---

## Tidy regression output

Achieved with functions from the `broom` package:

- .vocab[`tidy`]: Constructs a data frame that summarizes the model's statistical findings. We've talked about coefficient estimates and standard errors, but it also displays *test statistics and p-values*

- .vocab[`augment`]: Adds columns to the original data that was modeled. This includes predictions and residuals.

- .vocab[`glance`]: Constructs a concise one-row summary of the model, computed once for the entire model. 

---

## Tidy your model's statistical findings

```{r}
tidy(mod_years)
```

---

## Tidy your model's statistical findings


```{r}
tidy(mod_years) %>%
  select(term, estimate) %>%
  mutate(estimate = round(estimate, 3))
```